{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.utils\n",
    "import torch.random\n",
    "import torch.optim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torchvision.transforms.transforms as tvff\n",
    "import torchvision.datasets as tvd\n",
    "import torchvision.utils as tvu\n",
    "\n",
    "device = torch.device('cuda:7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tvd.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "xxx, yyy = data.train_data, data.train_labels\n",
    "\n",
    "xxx.unsqueeze_(1)\n",
    "yyy.unsqueeze_(1)\n",
    "\n",
    "nb_digits = 10\n",
    "yyy_onehot = torch.FloatTensor(yyy.shape[0], nb_digits)\n",
    "yyy_onehot.zero_()\n",
    "yyy_onehot.scatter_(1, yyy, 1)\n",
    "yyy = yyy_onehot\n",
    "\n",
    "xxx, yyy = xxx.type(torch.float32) / 255, yyy.type(torch.float32)\n",
    "\n",
    "#x, y = x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6000\n",
    "load_index = 0\n",
    "\n",
    "def get_data():\n",
    "    global batch_size, load_index, xxx, yyy, device\n",
    "    if load_index + batch_size > len(xxx):\n",
    "        load_index = 0\n",
    "        #perm = torch.randperm(len(xxx))\n",
    "        #xxx, yyy = xxx[perm], yyy[perm]\n",
    "    \n",
    "    data_xx, data_yy = xxx[load_index:load_index+batch_size], yyy[load_index:load_index+batch_size]\n",
    "    load_index += batch_size\n",
    "    \n",
    "    data_xx, data_yy = data_xx.to(device), data_yy.to(device)\n",
    "    return data_xx, data_yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = lambda x: torch.nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "s = torch.nn.functional.sigmoid\n",
    "r = torch.nn.functional.relu\n",
    "p = torch.nn.MaxPool2d(2)\n",
    "pad1 = torch.nn.ZeroPad2d(1)\n",
    "N_FEATURES = 9\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.ec1 = torch.nn.Conv2d(1, 4, kernel_size=5, padding=2)\n",
    "        self.ec2 = torch.nn.Conv2d(4, 4, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.ec3 = torch.nn.Conv2d(4, 8, kernel_size=5, padding=2)\n",
    "        self.ec4 = torch.nn.Conv2d(8, 16, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.ec5 = torch.nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
    "        self.ec6 = torch.nn.Conv2d(32, 32, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(32 * 3 * 3 + 10, 32 * 3)\n",
    "        self.fc2 = torch.nn.Linear(32 * 3, 32 * 3)\n",
    "        self.fc_mu = torch.nn.Linear(32 * 3, N_FEATURES)\n",
    "        self.fc_logvar = torch.nn.Linear(32 * 3, N_FEATURES)\n",
    "        \n",
    "    def forward(self, x, label):\n",
    "        global u, s, r, p, pad1\n",
    "        \n",
    "        x = r(self.ec1(x))\n",
    "        x = r(self.ec2(x))\n",
    "        x = p(x)\n",
    "\n",
    "        x = r(self.ec3(x))\n",
    "        x = r(self.ec4(x))\n",
    "        x = p(x)\n",
    "        \n",
    "        x = r(self.ec5(x))\n",
    "        x = r(self.ec6(x))\n",
    "        x = p(x)\n",
    "        \n",
    "        x = x.reshape(-1, 32 * 3 * 3)\n",
    "        x = r(self.fc1(torch.cat((x, label), dim=1)))\n",
    "        x = r(self.fc2(x))\n",
    "        \n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_in = torch.nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.dc1 = torch.nn.Conv2d(16, 16, kernel_size=5, padding=2)\n",
    "        self.dc2 = torch.nn.Conv2d(16, 8, kernel_size=5, padding=2)                                 \n",
    "                                         \n",
    "        self.dc3 = torch.nn.Conv2d(8, 8, kernel_size=5, padding=2)\n",
    "        self.dc4 = torch.nn.Conv2d(8, 4, kernel_size=5, padding=2)     \n",
    "        \n",
    "        self.dc5 = torch.nn.Conv2d(4, 4, kernel_size=5, padding=2)\n",
    "        self.dc6 = torch.nn.Conv2d(4, 1, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.fc3 = torch.nn.Linear(10 + N_FEATURES, 32 * 3)\n",
    "        self.fc4 = torch.nn.Linear(32 * 3, 3 * 3 * 16)\n",
    "        \n",
    "    def forward(self, z, label):\n",
    "        global u, s, r, p, pad1\n",
    "        z = torch.cat((z, label), dim=1)\n",
    "        z = r(self.fc3(z))\n",
    "        z = r(self.fc4(z))\n",
    "        z = z.reshape(-1, 16, 3, 3)\n",
    "        \n",
    "        z = u(z)\n",
    "        z = r(self.dc1(z))\n",
    "        z = r(self.dc2(z))\n",
    "        \n",
    "        z = u(z)\n",
    "        z = pad1(z)\n",
    "        z = r(self.dc3(z))\n",
    "        z = r(self.dc4(z))\n",
    "        \n",
    "        z = u(z)\n",
    "        z = r(self.dc5(z))\n",
    "        z = s(self.dc6(z))\n",
    "        \n",
    "        return z\n",
    "        \n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.c0 = torch.nn.Conv2d(1, 8, kernel_size=7, stride=1, padding=3)\n",
    "        self.c1 = torch.nn.Conv2d(8, 8, kernel_size=7, stride=2, padding=3) # 14\n",
    "        self.c2 = torch.nn.Conv2d(8, 16, kernel_size=7, stride=2, padding=3) # 7\n",
    "        self.c3 = torch.nn.Conv2d(16, 32, kernel_size=3, padding=1) # 3\n",
    "        self.fc1 = torch.nn.Linear(32 * 3 * 3, 16 * 3)\n",
    "        self.fc2 = torch.nn.Linear(16 * 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c0(x)\n",
    "        x = self.c1(x)\n",
    "        x = self.c2(x)\n",
    "        \n",
    "        x_features = s(x.reshape(-1, 16 * 7 * 7))\n",
    "        \n",
    "        x = p(r(self.c3(x)))\n",
    "        #x_features = s(x.reshape(-1, 16 * 3 * 3))\n",
    "        \n",
    "        x = r(self.fc1(x.reshape(-1, 32 * 3 * 3)))\n",
    "        x = r(self.fc1(x.reshape(-1, 32 * 3 * 3)))\n",
    "        x = s(self.fc2(x))\n",
    "        return x_features, x\n",
    "\n",
    "def reparametrize(mu, logvar):\n",
    "    std = torch.exp(0.5*logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps*std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder().to(device)\n",
    "dec = Decoder().to(device)\n",
    "dis = Discriminator().to(device)\n",
    "\n",
    "enc_optimizer = torch.optim.Adam(enc.parameters(), 0.0001)\n",
    "dec_optimizer = torch.optim.Adam(dec.parameters(), 0.0001)\n",
    "#dis_optimizer = torch.optim.Adam(dis.parameters(), 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder(x_data, y_data):\n",
    "    global enc, dec, dis, enc_optimizer, dec_optimizer, dis_optimizer\n",
    "    enc_optimizer.zero_grad()\n",
    "    dec_optimizer.zero_grad()\n",
    "    dis_optimizer.zero_grad()\n",
    "\n",
    "    mu, logvar = enc(x_data, y_data)\n",
    "    code = reparametrize(mu, logvar)\n",
    "    x_recovered = dec(code, y_data)\n",
    "    \n",
    "    x_features, dis_result = dis(x_data)\n",
    "    x_recovered_features, dis_recovered_result = dis(x_recovered)\n",
    "    \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    BCE = (x_recovered_features - x_features).pow(2).sum() \n",
    "    \n",
    "    #torch.nn.functional.binary_cross_entropy(x_recovered, x_data, reduction='sum')\n",
    "    \n",
    "    loss_enc = KLD + BCE\n",
    "    loss_enc.backward()\n",
    "    \n",
    "    enc_optimizer.step()\n",
    "    \n",
    "    return loss_enc.item()\n",
    "    \n",
    "def train_decoder(x_data, y_data):\n",
    "    global enc, dec, dis, enc_optimizer, dec_optimizer, dis_optimizer\n",
    "    enc_optimizer.zero_grad()\n",
    "    dec_optimizer.zero_grad()\n",
    "    dis_optimizer.zero_grad()\n",
    "\n",
    "    mu, logvar = enc(x_data, y_data)\n",
    "    code = reparametrize(mu, logvar)\n",
    "    \n",
    "    \n",
    "    x_recovered = dec(code, y_data)\n",
    "    x_features, x_dis = dis(x_data)\n",
    "    \n",
    "    x_recovered_features, x_recovered_dis = dis(x_recovered)\n",
    "    \n",
    "    #KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    BCE = (x_recovered_features - x_features).pow(2).sum()\n",
    "    \n",
    "    x_sampled = dec(mu, y_data)\n",
    "    x_sampled_features, x_sampled_dis = dis(x_sampled)\n",
    "    \n",
    "    GAN = torch.nn.functional.binary_cross_entropy(x_dis, torch.zeros_like(x_dis)) + \\\n",
    "        torch.nn.functional.binary_cross_entropy(x_recovered_dis, torch.ones_like(x_recovered_dis)) + \\\n",
    "        torch.nn.functional.binary_cross_entropy(x_sampled_dis, torch.ones_like(x_sampled_dis))\n",
    "        \n",
    "    GAN = GAN.sum()\n",
    "    \n",
    "    loss_dec = BCE - GAN\n",
    "    loss_dec.backward()\n",
    "    dec_optimizer.step()\n",
    "    \n",
    "    return loss_dec.item()\n",
    "    \n",
    "def train_discriminator(x_data, y_data):\n",
    "    global enc, dec, enc_optimizer, dec_optimizer\n",
    "    enc_optimizer.zero_grad()\n",
    "    dec_optimizer.zero_grad()\n",
    "    dis_optimizer.zero_grad()\n",
    "\n",
    "    mu, logvar = enc(x_data, y_data)\n",
    "    code = reparametrize(mu, logvar)\n",
    "    x_recovered = dec(code, y_data)\n",
    "    \n",
    "    x_features, x_dis = dis(x_data)\n",
    "    x_recovered_features, x_recovered_dis = dis(x_recovered)\n",
    "    \n",
    "    x_sampled = dec(mu, y_data)\n",
    "    x_sampled_features, x_sampled_dis = dis(x_sampled)\n",
    "    \n",
    "    GAN = torch.nn.functional.binary_cross_entropy(x_dis, torch.zeros_like(x_dis)) + \\\n",
    "        torch.nn.functional.binary_cross_entropy(x_recovered_dis, torch.ones_like(x_recovered_dis)) + \\\n",
    "        torch.nn.functional.binary_cross_entropy(x_sampled_dis, torch.ones_like(x_sampled_dis))\n",
    "        \n",
    "    GAN = GAN.sum()\n",
    "    \n",
    "    loss_dis = GAN + (x_features.pow(2).sum(dim=-1) - 1).abs().sum() + \\\n",
    "            (x_sampled_features.pow(2).sum(dim=-1) - 1).abs().sum() + \\\n",
    "            (x_recovered_features.pow(2).sum(dim=-1) - 1).abs().sum()\n",
    "    \n",
    "    loss_dis.backward()\n",
    "    dis_optimizer.step()\n",
    "    \n",
    "    return loss_dis.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(x_data, y_data):\n",
    "    global enc, dec, dis, enc_optimizer, dec_optimizer, dis_optimizer\n",
    "    enc_optimizer.zero_grad()\n",
    "    dec_optimizer.zero_grad()\n",
    "    dis_optimizer.zero_grad()\n",
    "\n",
    "    mu, logvar = enc(x_data, y_data)\n",
    "    code = reparametrize(mu, logvar)\n",
    "       \n",
    "    x_features, x_dis = dis(x_data)\n",
    "    \n",
    "    x_recovered = dec(code, y_data)\n",
    "    x_recovered_features, x_recovered_dis = dis(x_recovered)\n",
    "    \n",
    "    x_sampled = dec(mu, y_data)\n",
    "    x_sampled_features, x_sampled_dis = dis(x_sampled)\n",
    "    \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    BCE = (x_recovered_features - x_features).pow(2).sum()\n",
    "    GAN = torch.nn.functional.binary_cross_entropy(x_dis, torch.zeros_like(x_dis), reduction='sum')\n",
    "    \n",
    "    loss_enc = KLD + BCE\n",
    "    loss_enc.backward(retain_graph=True)\n",
    "    enc_optimizer.step()\n",
    "    \n",
    "    loss_dec = 20 * BCE - GAN\n",
    "    loss_dec.backward(retain_graph=True)\n",
    "    dec_optimizer.step()\n",
    "    \n",
    "    loss_dis = GAN\n",
    "    loss_dis.backward(retain_graph=True)\n",
    "    dis_optimizer.step()\n",
    "    \n",
    "    \n",
    "    return loss_enc.item(), loss_dec.item(), loss_dis.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enc: 14021.179809570312\n",
      "Dec: 227188.51348876953\n",
      "Dis: 53234.84069824219\n",
      "------------------------------\n",
      "Enc: 3469.7980041503906\n",
      "Dec: 16406.199279785156\n",
      "Dis: 52989.652770996094\n",
      "------------------------------\n",
      "Enc: 1755.807071685791\n",
      "Dec: -17628.837432861328\n",
      "Dis: 52744.79150390625\n",
      "------------------------------\n",
      "Enc: 1078.4487962722778\n",
      "Dec: -30962.611206054688\n",
      "Dis: 52531.0\n",
      "------------------------------\n",
      "Enc: 733.2361268997192\n",
      "Dec: -37695.15899658203\n",
      "Dis: 52359.29296875\n",
      "------------------------------\n",
      "Enc: 530.6094303131104\n",
      "Dec: -41618.39685058594\n",
      "Dis: 52230.36785888672\n",
      "------------------------------\n",
      "Enc: 401.9226984977722\n",
      "Dec: -44100.33923339844\n",
      "Dis: 52138.68212890625\n",
      "------------------------------\n",
      "Enc: 314.45177483558655\n",
      "Dec: -45787.81524658203\n",
      "Dis: 52076.81555175781\n",
      "------------------------------\n",
      "Enc: 252.65333461761475\n",
      "Dec: -46984.27746582031\n",
      "Dis: 52037.209411621094\n",
      "------------------------------\n",
      "Enc: 206.98565459251404\n",
      "Dec: -47873.682189941406\n",
      "Dis: 52013.23858642578\n",
      "------------------------------\n",
      "Enc: 172.57481002807617\n",
      "Dec: -48548.28338623047\n",
      "Dis: 51999.550537109375\n",
      "------------------------------\n",
      "Enc: 145.74550664424896\n",
      "Dec: -49077.39044189453\n",
      "Dis: 51992.21746826172\n",
      "------------------------------\n",
      "Enc: 124.66226363182068\n",
      "Dec: -49495.52685546875\n",
      "Dis: 51988.564453125\n",
      "------------------------------\n",
      "Enc: 107.60926556587219\n",
      "Dec: -49834.89099121094\n",
      "Dis: 51986.898498535156\n",
      "------------------------------\n",
      "Enc: 93.78858292102814\n",
      "Dec: -50110.53497314453\n",
      "Dis: 51986.224853515625\n",
      "------------------------------\n",
      "Enc: 82.31875467300415\n",
      "Dec: -50339.848571777344\n",
      "Dis: 51986.00048828125\n",
      "------------------------------\n",
      "Enc: 72.79898303747177\n",
      "Dec: -50530.08203125\n",
      "Dis: 51985.95935058594\n",
      "------------------------------\n",
      "Enc: 64.7312096953392\n",
      "Dec: -50691.50811767578\n",
      "Dis: 51985.97155761719\n",
      "------------------------------\n",
      "Enc: 57.915454626083374\n",
      "Dec: -50827.78302001953\n",
      "Dis: 51985.99609375\n",
      "------------------------------\n",
      "Enc: 52.0361949801445\n",
      "Dec: -50945.375244140625\n",
      "Dis: 51986.01623535156\n",
      "------------------------------\n",
      "Enc: 47.00240868330002\n",
      "Dec: -51046.141052246094\n",
      "Dis: 51986.02453613281\n",
      "------------------------------\n",
      "Enc: 42.58967483043671\n",
      "Dec: -51134.34631347656\n",
      "Dis: 51986.03796386719\n",
      "------------------------------\n",
      "Enc: 38.75638657808304\n",
      "Dec: -51210.89044189453\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 35.3687579035759\n",
      "Dec: -51278.66180419922\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 32.39691188931465\n",
      "Dec: -51338.10656738281\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 29.738924831151962\n",
      "Dec: -51391.27362060547\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 27.387818843126297\n",
      "Dec: -51438.34704589844\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 25.262718230485916\n",
      "Dec: -51480.79022216797\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 23.371150940656662\n",
      "Dec: -51518.666748046875\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 21.6476269364357\n",
      "Dec: -51553.05694580078\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 20.105569064617157\n",
      "Dec: -51583.960510253906\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 18.69365695118904\n",
      "Dec: -51612.18328857422\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 17.41953355073929\n",
      "Dec: -51637.69940185547\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 16.247819989919662\n",
      "Dec: -51661.117431640625\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 15.183021008968353\n",
      "Dec: -51682.404235839844\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 14.203886672854424\n",
      "Dec: -51702.02380371094\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 13.304983481764793\n",
      "Dec: -51719.94519042969\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 12.476301655173302\n",
      "Dec: -51736.51940917969\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 11.717120453715324\n",
      "Dec: -51751.726318359375\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 11.010712549090385\n",
      "Dec: -51765.831115722656\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 10.363851517438889\n",
      "Dec: -51778.82434082031\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 9.7586210668087\n",
      "Dec: -51790.90655517578\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 9.201464414596558\n",
      "Dec: -51802.07843017578\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 8.6803168207407\n",
      "Dec: -51812.488830566406\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 8.194952458143234\n",
      "Dec: -51822.14813232422\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 7.743414156138897\n",
      "Dec: -51831.16455078125\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 7.325073413550854\n",
      "Dec: -51839.558532714844\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 6.931605540215969\n",
      "Dec: -51847.404724121094\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 6.567878901958466\n",
      "Dec: -51854.73095703125\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 6.22563149780035\n",
      "Dec: -51861.58624267578\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 5.903747029602528\n",
      "Dec: -51868.006103515625\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 5.602289862930775\n",
      "Dec: -51874.017578125\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 5.3195279240608215\n",
      "Dec: -51879.66345214844\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 5.052727282047272\n",
      "Dec: -51884.954528808594\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 4.807910557836294\n",
      "Dec: -51889.939514160156\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 4.572537247091532\n",
      "Dec: -51894.611877441406\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 4.351342715322971\n",
      "Dec: -51899.02453613281\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 4.146272510290146\n",
      "Dec: -51903.16125488281\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 3.9496161565184593\n",
      "Dec: -51907.07843017578\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 3.763294894248247\n",
      "Dec: -51910.75134277344\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 3.5892765633761883\n",
      "Dec: -51914.23913574219\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 3.427178166806698\n",
      "Dec: -51917.508544921875\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 3.2723031975328922\n",
      "Dec: -51920.622131347656\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 3.123952906578779\n",
      "Dec: -51923.54016113281\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 2.9881592951714993\n",
      "Dec: -51926.32745361328\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 2.857383321970701\n",
      "Dec: -51928.938415527344\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 2.731191262602806\n",
      "Dec: -51931.43896484375\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 2.6144522316753864\n",
      "Dec: -51933.780822753906\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 2.5006068907678127\n",
      "Dec: -51936.030517578125\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 2.3963095992803574\n",
      "Dec: -51938.13702392578\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 2.294238779693842\n",
      "Dec: -51940.16583251953\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 2.204794477671385\n",
      "Dec: -51942.064392089844\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 2.1091781314462423\n",
      "Dec: -51943.89794921875\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 2.0240725837647915\n",
      "Dec: -51945.61248779297\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 1.9376435708254576\n",
      "Dec: -51947.27282714844\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 1.8621154092252254\n",
      "Dec: -51948.825134277344\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 1.7875599022954702\n",
      "Dec: -51950.33233642578\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 1.7194832228124142\n",
      "Dec: -51951.73944091797\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 1.6496298592537642\n",
      "Dec: -51953.110412597656\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n",
      "Enc: 1.5857144948095083\n",
      "Dec: -51954.389221191406\n",
      "Dis: 51986.041259765625\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "enc = torch.load('enc.hht').to(device)\n",
    "dec = torch.load('dec.hht').to(device)\n",
    "#dis = torch.load('dis.hht').to(device)\n",
    "enc_optimizer = torch.optim.Adam(enc.parameters(), 0.0001)\n",
    "dec_optimizer = torch.optim.Adam(dec.parameters(), 0.0001)\n",
    "dis_optimizer = torch.optim.Adam(dis.parameters(), 0.0001, weight_decay=0.001)\n",
    "\n",
    "for _ in range(5000):\n",
    "    le = []\n",
    "    lc = [] \n",
    "    ld = []\n",
    "    \n",
    "    for __ in range(15):\n",
    "        #for i in range(6):\n",
    "        #    data_x, data_y = get_data()\n",
    "        #    l = train_encoder(data_x, data_y)\n",
    "        #    le.append(l)\n",
    "\n",
    "        #for i in range(6):\n",
    "        #    data_x, data_y = get_data()\n",
    "        #    l = train_decoder(data_x, data_y)\n",
    "        #    ld.append(l)\n",
    "\n",
    "        #for i in range(1):\n",
    "        #    data_x, data_y = get_data()\n",
    "        #    l = train_discriminator(data_x, data_y)\n",
    "        #    lc.append(l)\n",
    "            \n",
    "        for i in range(5):\n",
    "            data_x, data_y = get_data()\n",
    "            le1, ld1, lc1 = train(data_x, data_y)\n",
    "            le.append(le1)\n",
    "            ld.append(ld1)\n",
    "            lc.append(lc1)\n",
    "        \n",
    "    print('Enc:', sum(le))   \n",
    "    print('Dec:', sum(ld))    \n",
    "    print('Dis:', sum(lc))\n",
    "    print('-' * 30)\n",
    "    \n",
    "    torch.save(enc, 'enc.hht')\n",
    "    torch.save(dec, 'dec.hht')\n",
    "    torch.save(dis, 'dis.hht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = get_data()\n",
    "a.shape, b.shape\n",
    "pred = dec(enc(a[:100], b[:100])[0], b[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, 99)\n",
    "plt.imshow(a[i, 0, :, :].detach().cpu())\n",
    "plt.show()\n",
    "plt.imshow(pred[i, 0, :, :].detach().cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
